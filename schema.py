import logging
from typing_extensions import TypedDict
from typing import Optional, List
from pydantic import BaseModel, Field, validator

logger = logging.getLogger(__name__)

# --- 1. Graph State ---
class GraphState(TypedDict):
    """
    Represents the state of our graph.
    
    Attributes:
        paper_path: The path to the input research paper.
        recipe: The structured experiment parameters extracted by the researcher.
        generated_code: The Python code generated by the coder.
        docker_logs: Logs captured from the Docker build and run steps.
        error: An error message if a node fails.
        mlflow_run_id: The ID for the current MLflow experiment run.
        retries: The number of times the coder has tried to fix the code.
    """
    paper_path: str
    recipe: Optional[dict]
    generated_code: Optional[str]
    dockerfile_content: Optional[str]
    docker_logs: Optional[str]
    error: Optional[str]
    mlflow_run_id: Optional[str]
    retries: int

class LibraryRequirements(BaseModel):
    """Pydantic model for extracting required libraries."""
    libraries: List[str] = Field(description="A list of pip installable libraries, e.g., ['timm', 'opencv-python']")
    
    @validator("libraries")
    def validate_libraries(cls, v):
        """Ensure libraries are simple strings."""
        if not all(isinstance(lib, str) and " " not in lib for lib in v):
            logger.warning(f"Invalid library list from LLM: {v}")
            return ["transformers", "torch", "datasets"]
        return v
    
# --- 2. Pydantic Models for LLM Output ---
class ExperimentRecipe(BaseModel):
    """Pydantic model for extracting structured data from a research paper."""
    model_architecture: str = Field(description="e.g., 'BERT Transformer', 'ResNet-50'")
    dataset: str = Field(description="e.g., 'IMDB', 'ImageNet', 'Custom Dataset'")
    optimizer: str = Field(description="e.g., 'AdamW', 'SGD'")
    learning_rate: float = Field(description="e.g., 2e-5, 0.001")
    batch_size: int = Field(description="e.g., 16, 64")
    loss_function: str = Field(description="e.g., 'Binary Cross-Entropy', 'Categorical Cross-Entropy'")
    gpu_required: bool = Field(description="True if a GPU is mentioned as necessary, otherwise False")

    @validator("learning_rate")
    def validate_learning_rate(cls, v):
        """Ensure learning rate is a sensible positive number."""
        try:
            if v <= 0:
                logger.warning(f"Invalid learning_rate '{v}' from LLM, defaulting to 0.001")
                return 0.001
        except Exception:
            logger.warning(f"Invalid learning_rate '{v}' from LLM, defaulting to 0.001")
            return 0.001
        return v

    @validator("batch_size")
    def validate_batch_size(cls, v):
        """Ensure batch size is a sensible positive number."""
        try:
            if v <= 0:
                logger.warning(f"Invalid batch_size '{v}' from LLM, defaulting to 32")
                return 32
        except Exception:
            logger.warning(f"Invalid batch_size '{v}' from LLM, defaulting to 32")
            return 32
        return v

    @validator("model_architecture", "dataset", "optimizer", "loss_function")
    def validate_strings(cls, v):
        """Ensure strings are not empty and don't start with problematic characters."""
        v = str(v).strip()
        if not v or v.startswith("-") or "not mentioned" in v.lower() or "not specified" in v.lower():
            logger.warning(f"Invalid string '{v}' from LLM, defaulting to 'N/A'")
            return "N/A"
        return v
    
    @validator("optimizer")
    def validate_optimizer(cls, v):
        """Clean and validate optimizer name."""
        v = str(v).strip()
        
        # Common optimizer names
        valid_optimizers = ["adam", "adamw", "sgd", "rmsprop", "adagrad"]
        
        # Check if it's a description instead of an optimizer name
        if len(v) > 50 or "document" in v.lower() or "provide" in v.lower():
            logger.warning(f"Invalid optimizer description: '{v[:50]}...', defaulting to Adam")
            return "Adam"
        
        # Extract optimizer name if buried in text
        v_lower = v.lower()
        for opt in valid_optimizers:
            if opt in v_lower:
                return opt.capitalize()  # Return "Adam", "Sgd", etc.
        
        # If still unclear, default to Adam
        if not v or v == "N/A":
            logger.warning(f"No optimizer found, defaulting to Adam")
            return "Adam"
        
        return v

